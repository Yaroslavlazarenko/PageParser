# api_parser.py
import asyncio
import re
from bs4 import BeautifulSoup
import aiohttp
from typing import List, Dict, Optional
import json
from datetime import datetime
import aiofiles

# --- –ö–ª–∞—Å—Å—ã –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
class APIError(Exception):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –æ—à–∏–±–æ–∫ API abit-poisk."""
    pass
# ... (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –æ—à–∏–±–æ–∫ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
class APIRateLimitError(APIError):
    def __init__(self, message="–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –æ—à–∏–±–∫–∞ 'max_user_connections'"):
        self.message = message
        super().__init__(self.message)
class APIUnavailableError(APIError):
    def __init__(self, status_code: int, reason: str):
        self.message = f"API –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –°—Ç–∞—Ç—É—Å: {status_code} {reason}"
        super().__init__(self.message)
class APIInvalidResponseError(APIError):
    def __init__(self, message="–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–ª–∏ –Ω–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç API"):
        self.message = message
        super().__init__(self.message)
def _parse_delay_from_message(message: str) -> Optional[int]:
    match = re.search(r'—á–µ—Ä–µ–∑ (\d+)\s+—Å–µ–∫—É–Ω–¥', message)
    if match:
        try: return int(match.group(1))
        except (ValueError, IndexError): return None
    return None

LOG_FILE = 'debug_log.txt'
log_lock = asyncio.Lock()

async def log_to_file(content: str):
    async with log_lock:
        try:
            async with aiofiles.open(LOG_FILE, mode='a', encoding='utf-8') as f:
                await f.write(content)
        except Exception as e:
            print(f"!!! –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –ª–æ–≥-—Ñ–∞–π–ª: {e}")

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –ù–ê–•–£–ô –ì–õ–û–ë–ê–õ–¨–ù–£–Æ –°–ï–°–°–ò–Æ! –£–ù–ò–ß–¢–û–ñ–ê–ï–ú –£–õ–ò–ö–ò! ---
# AIOHTTP_SESSION = None  <--- –£–ë–†–ê–ù–û –ù–ê–•–£–ô
# async def get_session(): ... <--- –£–ë–†–ê–ù–û –ù–ê–•–£–ô
# async def close_session(): ... <--- –£–ë–†–ê–ù–û –ù–ê–•–£–ô

async def fetch_applications_html(search_query: str, semaphore: asyncio.Semaphore) -> Optional[str]:
    api_url = "http://abit-poisk.org.ua/api/statements/"
    headers = {
        'Host': 'abit-poisk.org.ua',
        'Sec-Ch-Ua-Platform': '"macOS"',
        'X-Requested-With': 'XMLHttpRequest',
        'Accept-Language': 'ru-RU,ru;q=0.9',
        'Sec-Ch-Ua': '"Chromium";v="139", "Not;A=Brand";v="99"',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Sec-Ch-Ua-Mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
        'Accept': '*/*',
        'Origin': 'http://abit-poisk.org.ua',
        'Sec-Fetch-Site': 'same-origin',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Dest': 'empty',
        'Referer': 'http://abit-poisk.org.ua/',
        'Accept-Encoding': 'gzip, deflate, br',
        'Priority': 'u=1, i',
        'Connection': 'keep-alive'
    }
    payload = {'search': search_query, 'offset': 0}
    all_html_parts = []
    
    # session = await get_session() # <--- –≠–¢–û–ô –•–£–ô–ù–ò –ë–û–õ–¨–®–ï –ù–ï–¢

    RETRY_DELAY = 20
    MAX_RETRIES = 5
    previous_count = -1

    while True:
        data = None
        is_successful_request = False
        last_exception = None
        
        attempt = 0
        while attempt < MAX_RETRIES:
            last_exception = None
            log_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
            log_parts = [f"\n{'='*120}\n", f"TRANSACTION AT: {log_timestamp} | Search: '{search_query}' | Offset: {payload.get('offset')} | Attempt: {attempt + 1}/{MAX_RETRIES}\n"]
            
            try:
                text_response, status_code, reason, response_headers = "", 0, "", {}
                async with semaphore:
                    log_parts.extend([f"{'-'*55} REQUEST {'-'*56}\n", f"URL: {api_url}\n", f"Headers: {json.dumps(headers, indent=2, ensure_ascii=False)}\n", f"Payload Body: {json.dumps(payload, indent=2, ensure_ascii=False)}\n"])
                    params = {'nocache': int(asyncio.get_event_loop().time() * 1000)}

                    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 2: –°–û–ó–î–ê–Å–ú, –ë–õ–Ø–î–¨, –ù–û–í–£–Æ –°–ï–°–°–ò–Æ –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ó–ê–ü–†–û–°–ê! ---
                    # –ö–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç-—Å–º–µ—Ä—Ç–Ω–∏–∫! üí•
                    connector = aiohttp.TCPConnector(ssl=False)
                    async with aiohttp.ClientSession(connector=connector) as session:
                        async with session.post(api_url, headers=headers, params=params, data=payload, timeout=30) as response:
                            text_response = await response.text()
                            status_code = response.status
                            reason = str(response.reason)
                            response_headers = dict(response.headers)
                            log_parts.extend([f"{'-'*54} RESPONSE {'-'*55}\n", f"Status: {status_code} {reason}\n", f"Headers: {json.dumps(response_headers, indent=2, ensure_ascii=False)}\n", f"Body:\n{text_response}\n"])
                            response.raise_for_status()

                # ... (–≤—Å—è –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
                if 'max_user_connections' in text_response: raise APIRateLimitError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ 'max_user_connections' –≤ —Ç–µ–ª–µ –æ—Ç–≤–µ—Ç–∞.")
                try: data = json.loads(text_response)
                except json.JSONDecodeError: raise APIInvalidResponseError(f"–û–∂–∏–¥–∞–ª—Å—è JSON, –Ω–æ –ø–æ–ª—É—á–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç. –ù–∞—á–∞–ª–æ: {text_response[:150]}")
                if not data.get('success', False):
                    error_message = data.get('message', '') or data.get('error', '')
                    if '–ß–∞—Å—Ç–æ—Ç–∞ –∑–∞–ø–∏—Ç—ñ–≤' in error_message: raise APIRateLimitError(f"API —Å–æ–æ–±—â–∏–ª –æ–± –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–∏: {error_message}")
                    else: raise APIInvalidResponseError(f"API –≤–µ—Ä–Ω—É–ª success=false: {error_message}")
                if data.get('count', 0) > 0 and not data.get('html'): raise APIInvalidResponseError("API —Å–æ–æ–±—â–∏–ª –æ –Ω–∞–ª–∏—á–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π HTML.")
                
                is_successful_request = True
                await log_to_file("".join(log_parts) + f"{'='*120}\n")
                break
            
            except (aiohttp.ClientError, asyncio.TimeoutError, APIError) as e:
                last_exception = e

            # ... (–≤—Å—è –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
            if isinstance(last_exception, APIRateLimitError):
                log_parts.extend([f"{'-'*54} EXCEPTION CAUGHT {'-'*48}\n", f"Error Details: {last_exception}\n"])
                current_delay = RETRY_DELAY
                parsed_delay = _parse_delay_from_message(last_exception.message)
                if parsed_delay is not None: current_delay = parsed_delay + 1
                log_parts.append(f"INFO: –û—à–∏–±–∫–∞ –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤. –û–∂–∏–¥–∞–Ω–∏–µ {current_delay} —Å–µ–∫. –ü–æ–ø—ã—Ç–∫–∞ –Ω–µ –∑–∞—Å—á–∏—Ç–∞–Ω–∞.\n")
                await log_to_file("".join(log_parts) + f"{'='*120}\n")
                await asyncio.sleep(current_delay)
            else:
                attempt += 1
                log_parts.extend([f"{'-'*54} EXCEPTION CAUGHT {'-'*48}\n", f"Error Details: {last_exception}\n"])
                if attempt < MAX_RETRIES:
                    log_parts.append(f"INFO: –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫.\n")
                    await log_to_file("".join(log_parts) + f"{'='*120}\n")
                    await asyncio.sleep(RETRY_DELAY)
                else:
                    await log_to_file("".join(log_parts) + f"{'='*120}\n")
        
        if not is_successful_request:
            raise last_exception

        if not (data and data.get('success') and data.get('html')):
            break
        
        all_html_parts.append(data['html'])
        total_count = data.get('count', 0)
        
        current_count = len(parse_applications("".join(all_html_parts)))
        if current_count >= total_count: break
        
        if current_count == previous_count and current_count > 0:
            await log_to_file(f"WARNING: –ü–∞–≥–∏–Ω–∞—Ü–∏—è –∑–∞—Å—Ç—Ä—è–ª–∞ –¥–ª—è '{search_query}' –Ω–∞ —Å–º–µ—â–µ–Ω–∏–∏ {current_count}. –ü—Ä–µ—Ä—ã–≤–∞—é —Ü–∏–∫–ª.\n")
            break

        previous_count = current_count
        payload['offset'] = current_count
        await asyncio.sleep(0.5)
            
    return "".join(all_html_parts) if all_html_parts else None


def parse_applications(html_content: str) -> List[Dict]:
    # ... (–∫–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –æ–Ω –æ—Ö—É–µ–Ω–µ–Ω –∫–∞–∫ –µ—Å—Ç—å) ...
    soup = BeautifulSoup(html_content, 'lxml'); applications = []; base_url = "https://abit-poisk.org.ua"
    table_bodies = soup.find_all('tbody')
    if not table_bodies: return []
    for table_body in table_bodies:
        for row in table_body.find_all('tr'):
            cells = row.find_all('td');
            if len(cells) < 14: continue
            try:
                rank_position_text = cells[3].get_text(strip=True); rank_position = int(rank_position_text) if rank_position_text.isdigit() else 0
                total_score_text = cells[6].get_text(strip=True).replace(',', '.'); total_score = float(total_score_text) if total_score_text else 0.0
                scores = {}; score_components_cell = cells[8]; subjects = score_components_cell.find_all('dt'); points = score_components_cell.find_all('dd')
                for subject, point in zip(subjects, points):
                    subject_name = subject.get_text(strip=True); score_value = point.get_text(strip=True)
                    scores[subject_name] = int(score_value) if score_value.isdigit() else score_value
                coefficients = {}; coeffs_list = score_components_cell.find('ul', class_='list-unstyled')
                if coeffs_list:
                    for li in coeffs_list.find_all('li'):
                        coeff_text = li.get_text(strip=True)
                        if ':' in coeff_text: key, value = coeff_text.split(':', 1); coefficients[key.strip()] = value.strip()
                places_info = {}; places_cell = cells[5]
                for span in places_cell.find_all('span'):
                    text = span.get_text(strip=True); tooltip = span.get('data-stooltip', '').lower()
                    numbers = re.findall(r'\d+', text)
                    if not numbers: continue
                    number = int(numbers[0])
                    if '–≤–º' in text.lower() or '–∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å' in tooltip: places_info['total'] = number
                    elif '–±–º' in text.lower() or '–±—é–¥–∂–µ—Ç–Ω–∏—Ö' in tooltip: places_info['budget_max'] = number
                    elif '–∫' in text.lower() or '–∫–æ–Ω—Ç—Ä–∞–∫—Ç' in tooltip: places_info['contract'] = number
                specialty_divs = cells[11].find_all('div'); specialty_full = specialty_divs[0].get_text(strip=True) if specialty_divs else ''
                specialty_code = ''; specialty_name = ''
                if specialty_full:
                    match = re.match(r'^([A-Z0-9]+)(.*)', specialty_full)
                    if match: specialty_code = match.group(1); specialty_name = match.group(2).strip()
                    else: specialty_name = specialty_full
                specialization = specialty_divs[1].get_text(strip=True) if len(specialty_divs) > 1 else ''
                rank_url_tag = cells[3].find('a'); rank_url = base_url + rank_url_tag.get('href', '') if rank_url_tag else ''
                university_url_tag = cells[9].find('a'); university_url = base_url + university_url_tag.get('href', '') if university_url_tag else ''
                application_data = {'degree_level_short': cells[0].find('div').get_text(strip=True), 'degree_level_full': cells[0].get('title', '').strip(),'applicant_name': cells[1].get_text(strip=True), 'status': cells[2].get_text(strip=True),'rank_position': rank_position, 'rank_url': rank_url,'priority': ' '.join(cells[4].get_text(strip=True).split()), 'places': places_info, 'total_score': total_score,'avg_document_score': cells[7].get_text(strip=True), 'score_components': scores, 'coefficients': coefficients,'university_name': cells[9].get_text(strip=True), 'university_url': university_url,'faculty_short': cells[10].get_text(strip=True), 'faculty_full': cells[10].get('title', '').strip(),'specialty_code': specialty_code, 'specialty_name': specialty_name, 'specialization': specialization,'quota': cells[12].get_text(strip=True), 'originals_submitted': cells[13].get_text(strip=True) == '+'}
                applications.append(application_data)
            except Exception: continue
    return applications